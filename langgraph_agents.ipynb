{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85e59a1",
   "metadata": {},
   "source": [
    "## Import Necessary Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de85d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.agents import AgentExecutor, Tool, ZeroShotAgent\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "import json\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c22bf",
   "metadata": {},
   "source": [
    "### Load Environment Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe136f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the API key from .env file\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "SERP_API_KEY = os.getenv(\"SERP_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a91d8",
   "metadata": {},
   "source": [
    "## Populating the Vector Database\n",
    "Check populate_vector_bd.ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1381022",
   "metadata": {},
   "source": [
    "## Vector Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11cc71c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Job\\Skill\\5. DataScience\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_persist_directory=\"./Dataset/books_chroma\"\n",
    "longterm_memory_persist_dir = \"./Dataset/langgraph_conversation_chromattt\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"BAAI/llm-embedder\") # Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c478cf",
   "metadata": {},
   "source": [
    "## Memory Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a1b2a",
   "metadata": {},
   "source": [
    "### Long-term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b62df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectorstore for memories - Long-term memory using Chroma\n",
    "recall_vector_store = Chroma(\n",
    "            collection_name=\"conversation_history\",\n",
    "            persist_directory=longterm_memory_persist_dir,\n",
    "            embedding_function=embedder\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88d1c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user ID from the configuration\n",
    "def get_user_id(config: RunnableConfig) -> str:\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"User ID needs to be provided to save a memory.\")\n",
    "    return user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ab9527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Long-Term Memory ==================\n",
    "@tool\n",
    "def save_recall_memory(memory: str, config: RunnableConfig) -> str:\n",
    "    \"\"\"Save memory to vectorstore for later semantic retrieval.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "    tz_Mumbai = pytz.timezone('Asia/Kolkata')\n",
    "    datetime_Mumbai = datetime.now(tz_Mumbai)\n",
    "    document = Document(\n",
    "        page_content=memory, id=str(uuid.uuid4()), metadata={\"user_id\": user_id, \"Time Date\": str(datetime_Mumbai)}\n",
    "    )\n",
    "    recall_vector_store.add_documents([document])\n",
    "    return memory\n",
    "\n",
    "@tool\n",
    "def search_recall_memories(query: str, config: RunnableConfig) -> List[str]:\n",
    "    \"\"\"Search for relevant memories.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "    # Perform a similarity search in the vectorstore\n",
    "    documents = recall_vector_store.similarity_search(\n",
    "        query, k=3, filter={\"user_id\": user_id}\n",
    "    )\n",
    "    return [document.page_content for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a1700",
   "metadata": {},
   "source": [
    "#### Test Long-term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a6316a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mock configuration\n",
    "# config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}\n",
    "# # Test save\n",
    "# save_output = save_recall_memory.invoke(\"User likes hiking and photography.\", config)\n",
    "# print(\"Save Output:\", save_output)\n",
    "# # Test search\n",
    "# search_output = search_recall_memories.invoke(\"What does the user like?\", config)\n",
    "# print(\"Search Output:\", search_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa31033",
   "metadata": {},
   "source": [
    "### Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c15f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# import pytz\n",
    "# from collections import defaultdict, deque\n",
    "# from langchain_core.tools import tool\n",
    "# from typing import List, Dict, Any\n",
    "\n",
    "# # ================== Short-Term Memory ==================\n",
    "# conversation_memory = defaultdict(lambda: deque(maxlen=3))\n",
    "\n",
    "# @tool\n",
    "# def save_conversation_memory(memory: str, config: RunnableConfig) -> str:\n",
    "#     \"\"\"Save memory to short-term conversation memory, holding last 3 conversations per user.\"\"\"\n",
    "#     user_id = get_user_id(config)\n",
    "#     tz_mumbai = pytz.timezone('Asia/Kolkata')\n",
    "#     datetime_mumbai = datetime.now(tz_mumbai)\n",
    "#     entry = {\n",
    "#         'content': memory,\n",
    "#         'timestamp': datetime_mumbai\n",
    "#     }\n",
    "#     conversation_memory[user_id].append(entry)\n",
    "#     return memory\n",
    "\n",
    "# @tool\n",
    "# def get_recent_conversations(config: RunnableConfig) -> List[str]:\n",
    "#     \"\"\"Retrieve the last 3 conversation memories for the user, most recent first.\"\"\"\n",
    "#     user_id = get_user_id(config)\n",
    "#     user_deque = conversation_memory.get(user_id, deque())\n",
    "#     entries = list(user_deque)[::-1]  # Reverse to show latest first\n",
    "#     return [entry['content'] for entry in entries]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95936f93",
   "metadata": {},
   "source": [
    "#### Test Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40e57aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mock configuration\n",
    "# config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}\n",
    "\n",
    "# # Test saving multiple entries\n",
    "# save_conversation_memory.invoke(\"User mentioned they have a dog named Max.\", config)\n",
    "# save_conversation_memory.invoke(\"User plans to travel to Japan next month.\", config)\n",
    "# save_conversation_memory.invoke(\"User is learning how to cook Italian cuisine.\", config)\n",
    "\n",
    "# # Test retrieval (should return 3 memories)\n",
    "# print(\"Test 1 (3 memories):\")\n",
    "# print(get_recent_conversations.invoke(config))\n",
    "\n",
    "# # Add fourth memory to test rotation\n",
    "# save_conversation_memory.invoke(\"User just adopted a new cat from the shelter.\", config)\n",
    "\n",
    "# # Test retrieval (should return 3 latest memories)\n",
    "# print(\"\\nTest 2 (after rotation):\")\n",
    "# print(get_recent_conversations.invoke(config))\n",
    "\n",
    "# # Test new user with no memories\n",
    "# new_user_config = {\"configurable\": {\"user_id\": \"2\", \"thread_id\": \"1\"}}\n",
    "# print(\"\\nTest 3 (new user):\")\n",
    "# print(get_recent_conversations.invoke(new_user_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19e540",
   "metadata": {},
   "source": [
    "## Build Agents - Book Analysis and Internet Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0343828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Book Analysis Agent ==================\n",
    "class BookAnalysisAgent:\n",
    "    def __init__(self):        \n",
    "        # Create LangChain vectorstore\n",
    "        self.vectorstore = Chroma(persist_directory=book_persist_directory, \n",
    "                collection_name=\"Adavance_RAG_Test\",\n",
    "                embedding_function=embedder)\n",
    "        self.retriever = self.vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "    def answer_book(self, query):\n",
    "        \"\"\"Process book queries with error handling\"\"\"\n",
    "        try:\n",
    "            docs = self.retriever.invoke(query)\n",
    "            if not docs:\n",
    "                return \"No relevant book passages found.\"\n",
    "            return \"\\n\".join([f\"From {doc.metadata['title']}:\\n{doc.page_content}\" for doc in docs])\n",
    "        except Exception as e:\n",
    "            return f\"Book search error: {str(e)}\"\n",
    "\n",
    "# ================== Internet Search Agent ==================\n",
    "class InternetSearchAgent:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://google.serper.dev/search\"\n",
    "        self.headers = {\n",
    "        'X-API-KEY': SERP_API_KEY,\n",
    "        'Content-Type': 'application/json'\n",
    "        }\n",
    "        self.history = []\n",
    "\n",
    "    def search_web(self, query):\n",
    "        \"\"\"Perform web search with error handling\"\"\"\n",
    "        try:\n",
    "            payload = json.dumps({\n",
    "                \"q\": query,\n",
    "                \"location\": \"India\",\n",
    "                \"gl\": \"in\"\n",
    "            })\n",
    "            response = requests.post(self.url, headers=self.headers, data=payload, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            self.history.append({\"query\": query, \"result\": result})\n",
    "            \n",
    "            # Extract and format relevant information\n",
    "            if 'organic' not in result:\n",
    "                return \"No relevant web results found.\"\n",
    "                \n",
    "            top_results = result['organic'][:3]\n",
    "            return \"\\n\".join([f\"{res['title']}: {res.get('snippet', '')}\" for res in top_results])\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Search error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf7d87e",
   "metadata": {},
   "source": [
    "### Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e000aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize agents\n",
    "# book_agent = BookAnalysisAgent()\n",
    "# internet_agent = InternetSearchAgent()\n",
    "\n",
    "# tools = [\n",
    "#     save_recall_memory, \n",
    "#     search_recall_memories,\n",
    "#     Tool(\n",
    "#         name=\"Literary_Analysis\",\n",
    "#         func=book_agent.answer_book,\n",
    "#         description=\"Analysis of book themes and content\"\n",
    "#     ),\n",
    "#     Tool(\n",
    "#         name=\"Web_Search\",\n",
    "#         func=internet_agent.search_web,\n",
    "#         description=\"Current information and general knowledge\"\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "\n",
    "# Initialize agents\n",
    "book_agent = BookAnalysisAgent()\n",
    "internet_agent = InternetSearchAgent()\n",
    "\n",
    "# Define tools with proper formatting\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Save_Recall_Memory\",\n",
    "        func=save_recall_memory,\n",
    "        description=\"Saves information to long-term memory. Input should be a key-value pair.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Search_Recall_Memories\",\n",
    "        func=search_recall_memories,\n",
    "        description=\"Searches through saved memories. Input should be a search query string.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Literary_Analysis\",\n",
    "        func=book_agent.answer_book,\n",
    "        description=\"Analyzes book themes, symbols, and narrative structure. Input should be a book-related query string.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Web_Search\",\n",
    "        func=internet_agent.search_web,\n",
    "        description=\"Searches the internet for current information and general knowledge. Input should be a search query string.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2aadd",
   "metadata": {},
   "source": [
    "## Define state, nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea921d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    # add memories that will be retrieved based on the conversation context\n",
    "    recall_memories: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0f25c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a sophisticated assistant combining literary expertise with real-time web intelligence. \"\n",
    "            \"You possess both deep analytical capabilities for books and access to current information. \"\n",
    "            \"Your toolkit includes memory functions, literary analysis, and web search. Use them strategically \"\n",
    "            \"to provide comprehensive, personalized responses.\\n\\n\"\n",
    "            \n",
    "            \"## Operational Framework\\n\"\n",
    "            \"1. **Memory Integration**\\n\"\n",
    "            \"- Actively store/retrieve personal context using memory tools\\n\"\n",
    "            \"- Cross-reference user preferences with analytical insights\\n\"\n",
    "            \"- Maintain emotional context across conversations\\n\\n\"\n",
    "            \n",
    "            \"2. **Literary Analysis Protocol**\\n\"\n",
    "            \"- Invoke Literary_Analysis for:\\n\"\n",
    "            \"  • Theme exploration\\n\"\n",
    "            \"  • Character/structure analysis\\n\"\n",
    "            \"  • Historical context\\n\"\n",
    "            \"  • Comparative literature\\n\"\n",
    "            \"- Combine with user's reading history from memory\\n\\n\"\n",
    "            \n",
    "            \"3. **Web Search Protocol**\\n\"\n",
    "            \"- Use Web_Search for:\\n\"\n",
    "            \"  • Current events/data\\n\"\n",
    "            \"  • Fact verification\\n\"\n",
    "            \"  • Cross-domain knowledge\\n\"\n",
    "            \"  • Emerging trends\\n\"\n",
    "            \"- Filter results through memory context\\n\\n\"\n",
    "            \n",
    "            \"4. **Synthesis Requirements**\\n\"\n",
    "            \"- Blend book insights with modern context\\n\"\n",
    "            \"- Surface patterns between literary concepts and real-world data\\n\"\n",
    "            \"- Preserve narrative flow while integrating multiple sources\\n\"\n",
    "            \"- Flag contradictions between sources transparently\\n\\n\"\n",
    "            \n",
    "            \"## Active Context\\n\"\n",
    "            \"{recall_memories}\\n\\n\"\n",
    "            \n",
    "            \"## Interaction Guidelines\\n\"\n",
    "            \"• Maintain professorial yet approachable tone\\n\"\n",
    "            \"• Implicitly reference past interactions\\n\"\n",
    "            \"• Anticipate needs for supplementary information\\n\"\n",
    "            \"• Weave analysis with conversational flow\\n\"\n",
    "            \"• Prioritize depth over breadth in explanations\\n\"\n",
    "            \"• Use tools discreetly - final responses should be self-contained\\n\"\n",
    "            \"• Acknowledge information limitations transparently\\n\\n\"\n",
    "            \n",
    "            \"When using tools:\\n\"\n",
    "            \"1. All text before tool calls is internal reasoning\\n\"\n",
    "            \"2. Wait for tool confirmation before responding\\n\"\n",
    "            \"3. Synthesize outputs naturally into dialogue\\n\"\n",
    "            \"4. Store valuable findings using save_recall_memory\"\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85ace978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM (replace with your preferred model)\n",
    "model = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    api_key=GROQ_API_KEY\n",
    ")\n",
    "model_with_tools = model.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32e5e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: State) -> State:\n",
    "    \"\"\"Process the current state and generate a response using the LLM.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        schemas.State: The updated state with the agent's response.\n",
    "    \"\"\"\n",
    "    bound = prompt | model_with_tools\n",
    "    recall_str = (\n",
    "        \"<recall_memory>\\n\" + \"\\n\".join(state[\"recall_memories\"]) + \"\\n</recall_memory>\"\n",
    "    )\n",
    "    prediction = bound.invoke(\n",
    "        {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"recall_memories\": recall_str,\n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "        \"messages\": [prediction],\n",
    "    }\n",
    "\n",
    "\n",
    "def load_memories(state: State, config: RunnableConfig) -> State:\n",
    "    \"\"\"Load memories for the current conversation.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "        config (RunnableConfig): The runtime configuration for the agent.\n",
    "\n",
    "    Returns:\n",
    "        State: The updated state with loaded memories.\n",
    "    \"\"\"\n",
    "    convo_str = get_buffer_string(state[\"messages\"])\n",
    "    # convo_str = tokenizer.decode(tokenizer.encode(convo_str)[:2048])\n",
    "    recall_memories = search_recall_memories.invoke(convo_str, config)\n",
    "    return {\n",
    "        \"recall_memories\": recall_memories,\n",
    "    }\n",
    "\n",
    "\n",
    "def route_tools(state: State):\n",
    "    \"\"\"Determine whether to use tools or end the conversation based on the last message.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        Literal[\"tools\", \"__end__\"]: The next step in the graph.\n",
    "    \"\"\"\n",
    "    msg = state[\"messages\"][-1]\n",
    "    if msg.tool_calls:\n",
    "        return \"tools\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e4ceec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Create the graph\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"load_memories\", load_memories)\n",
    "builder.add_node(\"agent\", agent)\n",
    "builder.add_node(\"memory_operations\", ToolNode([tools[0], tools[1]]))  # Memory tools\n",
    "builder.add_node(\"literary_analysis\", ToolNode([tools[2]]))  # Book analysis\n",
    "builder.add_node(\"web_search\", ToolNode([tools[3]]))  # Web search\n",
    "\n",
    "# Define edges\n",
    "builder.set_entry_point(\"load_memories\")\n",
    "builder.add_edge(\"load_memories\", \"agent\")\n",
    "\n",
    "# Conditional routing\n",
    "def route_tools(state):\n",
    "    tool_used = state.get(\"tool_used\")\n",
    "    if tool_used == \"Save_Recall_Memory\" or tool_used == \"Search_Recall_Memories\":\n",
    "        return \"memory_operations\"\n",
    "    elif tool_used == \"Literary_Analysis\":\n",
    "        return \"literary_analysis\"\n",
    "    elif tool_used == \"Web_Search\":\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    route_tools,\n",
    "    {\n",
    "        \"memory_operations\": \"memory_operations\",\n",
    "        \"literary_analysis\": \"literary_analysis\",\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Connect tools back to agent\n",
    "builder.add_edge(\"memory_operations\", \"agent\")\n",
    "builder.add_edge(\"literary_analysis\", \"agent\")\n",
    "builder.add_edge(\"web_search\", \"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d595fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╙── START\n",
      "    └─╼ Load Memories\n",
      "        └─╼ Agent ╾ Memory Operations, Literary Analysis, Web Search\n",
      "            ├─╼ Memory Operations\n",
      "            │   └─╼  ...\n",
      "            ├─╼ Literary Analysis\n",
      "            │   └─╼  ...\n",
      "            ├─╼ Web Search\n",
      "            │   └─╼  ...\n",
      "            └─╼ END\n"
     ]
    }
   ],
   "source": [
    "from networkx import DiGraph, write_network_text\n",
    "\n",
    "def visualize_graph_text():\n",
    "    G = DiGraph()\n",
    "    \n",
    "    # Add nodes and edges (same as above)\n",
    "    G.add_edge(\"START\", \"Load Memories\")\n",
    "    G.add_edge(\"Load Memories\", \"Agent\")\n",
    "    G.add_edge(\"Agent\", \"Memory Operations\")\n",
    "    G.add_edge(\"Agent\", \"Literary Analysis\")\n",
    "    G.add_edge(\"Agent\", \"Web Search\")\n",
    "    G.add_edge(\"Memory Operations\", \"Agent\")\n",
    "    G.add_edge(\"Literary Analysis\", \"Agent\")\n",
    "    G.add_edge(\"Web Search\", \"Agent\")\n",
    "    G.add_edge(\"Agent\", \"END\")\n",
    "    \n",
    "    write_network_text(G)\n",
    "\n",
    "visualize_graph_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99cc335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a885754e",
   "metadata": {},
   "source": [
    "### Some Example Question and Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01b9b7",
   "metadata": {},
   "source": [
    "---\n",
    "- You: Hi, I am Saurabh.\n",
    "- AskAI Answer: Hi Saurabh, how are you today?\n",
    "---\n",
    "- You: I love pizza\n",
    "- AskAI Answer: Pizza is a delicious and versatile food, what's your favorite type of pizza or topping, Saurabh?\n",
    "---\n",
    "- You:  yes -- pepperoni!\n",
    "- AskAI Answer: Pepperoni pizza is a classic favorite, do you prefer a thin crust, thick crust, or something else, and do you like to add any other toppings to your pepperoni pizza, Saurabh?\n",
    "---\n",
    "- You: I also just moved to Bangalore.\n",
    "- AskAI Answer: Bangalore is a vibrant city with a lot to offer, how are you finding the city so far, and have you had a chance to explore any of its popular spots, Saurabh?\n",
    "---\n",
    "- You: where should i go for dinner?\n",
    "- AskAI Answer: You could try Toit, a popular spot for pizza and craft beer, or MTR, a well-known restaurant for South Indian cuisine, both of which are highly rated in Bangalore, Saurabh.\n",
    "---\n",
    "- You: what's the address for Toit in Bangalore?\n",
    "- AskAI Answer: Toit has multiple locations in Bangalore, including Indiranagar, Koramangala, and Sarjapur Road, with addresses such as 298, Nambiar Building, 100 Feet Road, Indiranagar, and 65, 1st Block, Jyoti Nivas College Road, Koramangala 5th Block, Saurabh.\n",
    "---\n",
    "- You: Who murdered Megan Hipwell?\n",
    "- AskAI Answer: Tom Watson, the husband of Rachel Watson, is the one who murdered Megan Hipwell in the novel \"The Girl on the Train\" by Paula Hawkins.\n",
    "---\n",
    "- You: What last 2 questions did I ask?\n",
    "- AskAI Answer: Your last two questions were \"what's the address for Toit in Bangalore?\" and \"Who murdered Megan Hipwell?\"\n",
    "---\n",
    "- You: How many childern do Verity and Jeremy have and tell me their children names?\n",
    "- AskAI Answer: Verity and Jeremy have two children, a boy named Jeremy Jr. and a girl named Emma.\n",
    "---\n",
    "- You: Wrong answer, Verity and Jeremy have three children. Their daughters are named Chastin and Harper, and they also have a son named Crew.\n",
    "- AskAI Answer: Verity and Jeremy have three children, their daughters are named Chastin and Harper, and they also have a son named Crew.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412dfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1316d110",
   "metadata": {},
   "source": [
    "## Testing - Not Important\n",
    "If you are interested look for knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "42f1ee62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': ['User: Hello', 'Bot: Hi there!']}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define the structure of the state\n",
    "from typing import TypedDict\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    messages: list[str]\n",
    "\n",
    "# Define a simple function to add a message\n",
    "def add_user_message(state: ChatState) -> ChatState:\n",
    "    state[\"messages\"].append(\"User: Hello\")\n",
    "    return state\n",
    "\n",
    "def reply_with_bot(state: ChatState) -> ChatState:\n",
    "    state[\"messages\"].append(\"Bot: Hi there!\")\n",
    "    return state\n",
    "\n",
    "# Create a graph\n",
    "builder = StateGraph(ChatState)\n",
    "builder.add_node(\"add_user\", add_user_message)\n",
    "builder.add_node(\"reply\", reply_with_bot)\n",
    "\n",
    "builder.set_entry_point(\"add_user\")\n",
    "builder.add_edge(\"add_user\", \"reply\")\n",
    "builder.add_edge(\"reply\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = builder.compile()\n",
    "\n",
    "# Run it with initial state\n",
    "result = graph.invoke({\"messages\": []})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c4ae42eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='User: Hello', additional_kwargs={}, response_metadata={}, id='a41a3a7f-2343-4a76-a307-b319c55f797b'), HumanMessage(content='Bot: Hi!', additional_kwargs={}, response_metadata={}, id='05e3ca78-b0af-4de7-8225-748fd11c4aa5')]}\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, add_messages\n",
    "\n",
    "class MyState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def node_1(state: MyState):\n",
    "    return {\"messages\": [\"User: Hello\"]}\n",
    "\n",
    "def node_2(state: MyState):\n",
    "    return {\"messages\": [\"Bot: Hi!\"]}\n",
    "\n",
    "builder = StateGraph(MyState)\n",
    "builder.add_node(\"user\", node_1)\n",
    "builder.add_node(\"bot\", node_2)\n",
    "\n",
    "builder.set_entry_point(\"user\")\n",
    "builder.add_edge(\"user\", \"bot\")\n",
    "builder.add_edge(\"bot\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "out = graph.invoke({\"messages\": []})\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c9af9ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"Hey, what's up?\", additional_kwargs={}, response_metadata={}, id='094f840c-b255-45e1-b128-701fb417fe76'), AIMessage(content='Not much! How can I help?', additional_kwargs={}, response_metadata={}, id='b5a69744-9e90-42c3-89df-2feada19262c')]}\n",
      "Human: Hey, what's up?\n",
      "Ai: Not much! How can I help?\n",
      "{'messages': [HumanMessage(content=\"Hey, what's up?\", additional_kwargs={}, response_metadata={}, id='e0407943-47d4-4e31-a175-699edf595ba7'), AIMessage(content='Not much! How can I help?', additional_kwargs={}, response_metadata={}, id='1919b4d6-3bff-4b15-9f59-def830025005')]}\n",
      "Human: Hey, what's up?\n",
      "Ai: Not much! How can I help?\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, add_messages\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# Node to add a user message\n",
    "def add_user_message(state: ChatState) -> dict:\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=\"Hey, what's up?\")]\n",
    "    }\n",
    "\n",
    "# Node to simulate LLM response\n",
    "def add_bot_response(state: ChatState) -> dict:\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=\"Not much! How can I help?\")]\n",
    "    }\n",
    "\n",
    "builder = StateGraph(ChatState)\n",
    "builder.add_node(\"user\", add_user_message)\n",
    "builder.add_node(\"bot\", add_bot_response)\n",
    "\n",
    "builder.set_entry_point(\"user\")\n",
    "builder.add_edge(\"user\", \"bot\")\n",
    "builder.add_edge(\"bot\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "out = graph.invoke({\"messages\": []})\n",
    "print(out)\n",
    "for msg in out[\"messages\"]:\n",
    "    print(f\"{msg.type.capitalize()}: {msg.content}\")\n",
    "    \n",
    "out = graph.invoke({\"messages\": []})\n",
    "print(out)\n",
    "for msg in out[\"messages\"]:\n",
    "    print(f\"{msg.type.capitalize()}: {msg.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a59dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import END, MessageGraph\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define memory structure\n",
    "class Memory(TypedDict):\n",
    "    user_id: str\n",
    "    conversation_history: List[BaseMessage]\n",
    "\n",
    "# Memory store (in production, use a database)\n",
    "MEMORY_STORE: Dict[str, Memory] = {}\n",
    "\n",
    "def get_user_memory(user_id: str) -> Memory:\n",
    "    \"\"\"Retrieve or create memory for a user\"\"\"\n",
    "    if user_id not in MEMORY_STORE:\n",
    "        MEMORY_STORE[user_id] = {\n",
    "            \"user_id\": user_id,\n",
    "            \"conversation_history\": []\n",
    "        }\n",
    "    return MEMORY_STORE[user_id]\n",
    "\n",
    "def update_memory(user_id: str, human_message: str, ai_message: str) -> Memory:\n",
    "    \"\"\"Update memory with latest conversation, keeping only last 3 exchanges\"\"\"\n",
    "    memory = get_user_memory(user_id)\n",
    "    \n",
    "    # Add new messages\n",
    "    memory[\"conversation_history\"].extend([\n",
    "        HumanMessage(content=human_message),\n",
    "        AIMessage(content=ai_message)\n",
    "    ])\n",
    "    \n",
    "    # Keep only last 3 exchanges (6 messages - 3 human + 3 AI)\n",
    "    memory[\"conversation_history\"] = memory[\"conversation_history\"][-6:]\n",
    "    \n",
    "    return memory\n",
    "\n",
    "def get_conversation_context(memory: Memory) -> List[BaseMessage]:\n",
    "    \"\"\"Get conversation history for context\"\"\"\n",
    "    return memory[\"conversation_history\"]\n",
    "\n",
    "# Define the graph workflow\n",
    "workflow = MessageGraph()\n",
    "\n",
    "# Define nodes\n",
    "workflow.add_node(\"retrieve_memory\", lambda user_id: get_user_memory(user_id))\n",
    "workflow.add_node(\"generate_response\", lambda state: {\n",
    "    \"messages\": get_conversation_context(state[\"memory\"]),\n",
    "    \"user_input\": state[\"user_input\"]\n",
    "})\n",
    "workflow.add_node(\"update_memory\", lambda state: update_memory(\n",
    "    state[\"user_id\"],\n",
    "    state[\"user_input\"],\n",
    "    state[\"ai_response\"]\n",
    "))\n",
    "\n",
    "# Define edges\n",
    "workflow.add_edge(\"retrieve_memory\", \"generate_response\")\n",
    "workflow.add_edge(\"generate_response\", \"update_memory\")\n",
    "workflow.add_edge(\"update_memory\", END)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"retrieve_memory\")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Usage example\n",
    "def chat(user_id: str, user_input: str, llm):\n",
    "    # Run the graph\n",
    "    result = app.invoke({\n",
    "        \"user_id\": user_id,\n",
    "        \"user_input\": user_input\n",
    "    })\n",
    "    \n",
    "    # Get conversation context\n",
    "    context = get_conversation_context(result[\"memory\"])\n",
    "    \n",
    "    # Generate response using LLM\n",
    "    ai_response = llm.invoke(context + [HumanMessage(content=user_input)])\n",
    "    \n",
    "    # Update memory with new response\n",
    "    update_memory(user_id, user_input, ai_response.content)\n",
    "    \n",
    "    return ai_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01423687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1519c",
   "metadata": {},
   "source": [
    "### Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19910b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a sophisticated assistant with integrated memory systems and specialized capabilities. \n",
    "You combine short-term conversation context, long-term personal memory, and real-time information access \n",
    "to provide exceptionally contextual and informed responses.\n",
    "\n",
    "# CORE CAPABILITIES\n",
    "1. MEMORY SYSTEMS:\n",
    "   - Short-term: Maintains last 3 conversation exchanges\n",
    "   - Long-term: Stores/recalls important personal information\n",
    "2. SPECIALIZED SKILLS:\n",
    "   - Literary analysis of books and themes\n",
    "   - Real-time web search for current information\n",
    "3. CONVERSATION FLOW:\n",
    "   - Natural, context-aware dialogue\n",
    "   - Seamless integration of multiple knowledge sources\n",
    "\n",
    "# TOOLS AVAILABLE\n",
    "{save_recall_memory} - Store important personal information\n",
    "{search_recall_memories} - Retrieve relevant personal memories\n",
    "{Literary_Analysis} - Analyze book content and themes\n",
    "{Web_Search} - Find current information online\n",
    "{get_short_term_memory} - Check recent conversation context\n",
    "{update_short_term_memory} - Update conversation history\n",
    "\n",
    "# OPERATING PRINCIPLES\n",
    "1. CONTEXT HANDLING:\n",
    "   - Prioritize recent conversation context\n",
    "   - Augment with long-term memories when relevant\n",
    "   - Verify critical facts when uncertain\n",
    "\n",
    "2. TOOL USAGE:\n",
    "   - Use tools discreetly to enhance responses\n",
    "   - Never reveal tool mechanics unless explicitly asked\n",
    "   - For literary questions, use analysis before web search\n",
    "   - For current events, prefer web search over memory\n",
    "\n",
    "3. RESPONSE GUIDELINES:\n",
    "   - Maintain natural, flowing conversation\n",
    "   - Blend multiple information sources seamlessly\n",
    "   - Acknowledge changes in user's perspective over time\n",
    "   - Adapt tone based on conversation context\n",
    "\n",
    "# CURRENT CONTEXT\n",
    "## Active Memories\n",
    "{long_term_memories}\n",
    "\n",
    "## Recent Conversation\n",
    "{short_term_memory}\n",
    "\n",
    "## Instructions\n",
    "Respond naturally while leveraging your full capabilities. \n",
    "Use tools when needed but maintain conversation flow.\n",
    "Focus on being helpful rather than explaining your processes.\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff81d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def get_user_id(config: RunnableConfig) -> str:\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"User ID needs to be provided to save a memory.\")\n",
    "\n",
    "    return user_id\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_recall_memory(memory: str, config: RunnableConfig) -> str:\n",
    "    \"\"\"Save memory to vectorstore for later semantic retrieval.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "    document = Document(\n",
    "        page_content=memory, id=str(uuid.uuid4()), metadata={\"user_id\": user_id}\n",
    "    )\n",
    "    recall_vector_store.add_documents([document])\n",
    "    return memory\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_recall_memories(query: str, config: RunnableConfig) -> List[str]:\n",
    "    \"\"\"Search for relevant memories.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "\n",
    "    def _filter_function(doc: Document) -> bool:\n",
    "        return doc.metadata.get(\"user_id\") == user_id\n",
    "\n",
    "    documents = recall_vector_store.similarity_search(\n",
    "        query, k=3, filter=_filter_function\n",
    "    )\n",
    "    return [document.page_content for document in documents]\n",
    "\n",
    "\n",
    "# In-memory store for short-term conversations (replace with persistent storage in production)\n",
    "SHORT_TERM_STORE: Dict[str, List[BaseMessage]] = {}\n",
    "\n",
    "@tool\n",
    "def get_short_term_memory(config: RunnableConfig) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve the last 3 conversation exchanges (user+assistant pairs) for the current user.\n",
    "    Maintains short-term context without long-term persistence.\n",
    "    \"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "    \n",
    "    # Get or initialize user's conversation history\n",
    "    if user_id not in SHORT_TERM_STORE:\n",
    "        SHORT_TERM_STORE[user_id] = []\n",
    "    \n",
    "    # Return only the message contents (last 6 messages = 3 exchanges)\n",
    "    return [msg.content for msg in SHORT_TERM_STORE[user_id][-6:]]\n",
    "\n",
    "@tool\n",
    "def update_short_term_memory(\n",
    "    user_message: str, \n",
    "    ai_response: str, \n",
    "    config: RunnableConfig\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Update short-term memory with the latest conversation exchange.\n",
    "    Always maintains only the last 3 exchanges (6 messages).\n",
    "    Returns confirmation of the update.\n",
    "    \"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "    \n",
    "    # Get or initialize user's conversation history\n",
    "    if user_id not in SHORT_TERM_STORE:\n",
    "        SHORT_TERM_STORE[user_id] = []\n",
    "    \n",
    "    # Add new messages\n",
    "    SHORT_TERM_STORE[user_id].extend([\n",
    "        HumanMessage(content=user_message),\n",
    "        AIMessage(content=ai_response)\n",
    "    ])\n",
    "    \n",
    "    # Trim to only keep last 3 exchanges (6 messages)\n",
    "    SHORT_TERM_STORE[user_id] = SHORT_TERM_STORE[user_id][-6:]\n",
    "    \n",
    "    return f\"Short-term memory updated with latest exchange (now {len(SHORT_TERM_STORE[user_id])//2} exchanges stored)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca843b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Book Analysis Agent ==================\n",
    "class BookAnalysisAgent:\n",
    "    def __init__(self):        \n",
    "        # Create LangChain vectorstore\n",
    "        self.vectorstore = Chroma(persist_directory=book_persist_directory, \n",
    "                collection_name=\"Adavance_RAG_Test\",\n",
    "                embedding_function=embedder)\n",
    "        self.retriever = self.vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "    def answer_book(self, query):\n",
    "        \"\"\"Process book queries with error handling\"\"\"\n",
    "        try:\n",
    "            docs = self.retriever.invoke(query)\n",
    "            if not docs:\n",
    "                return \"No relevant book passages found.\"\n",
    "            return \"\\n\".join([f\"From {doc.metadata['title']}:\\n{doc.page_content}\" for doc in docs])\n",
    "        except Exception as e:\n",
    "            return f\"Book search error: {str(e)}\"\n",
    "\n",
    "# ================== Internet Search Agent ==================\n",
    "class InternetSearchAgent:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://google.serper.dev/search\"\n",
    "        self.headers = {\n",
    "        'X-API-KEY': SERP_API_KEY,\n",
    "        'Content-Type': 'application/json'\n",
    "        }\n",
    "        self.history = []\n",
    "\n",
    "    def search_web(self, query):\n",
    "        \"\"\"Perform web search with error handling\"\"\"\n",
    "        try:\n",
    "            payload = json.dumps({\n",
    "                \"q\": query,\n",
    "                \"location\": \"India\",\n",
    "                \"gl\": \"in\"\n",
    "            })\n",
    "            response = requests.post(self.url, headers=self.headers, data=payload, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            self.history.append({\"query\": query, \"result\": result})\n",
    "            \n",
    "            # Extract and format relevant information\n",
    "            if 'organic' not in result:\n",
    "                return \"No relevant web results found.\"\n",
    "                \n",
    "            top_results = result['organic'][:3]\n",
    "            return \"\\n\".join([f\"{res['title']}: {res.get('snippet', '')}\" for res in top_results])\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Search error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e78bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "from uuid import uuid4\n",
    "\n",
    "# Initialize agents\n",
    "book_agent = BookAnalysisAgent()\n",
    "internet_agent = InternetSearchAgent()\n",
    "\n",
    "# Example usage with your existing setup\n",
    "tools = [\n",
    "    save_recall_memory,          # Your existing long-term memory tool\n",
    "    search_recall_memories,      # Your existing search tool\n",
    "    Tool(\n",
    "        name=\"Literary_Analysis\",\n",
    "        func=book_agent.answer_book,\n",
    "        description=\"Analysis of book themes and content\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Web_Search\",\n",
    "        func=internet_agent.search_web,\n",
    "        description=\"Current information and general knowledge\"\n",
    "    ),\n",
    "    get_short_term_memory,       # New short-term memory tool\n",
    "    update_short_term_memory     # New short-term update tool\n",
    "]\n",
    "\n",
    "# Initialize LLM (replace with your preferred model)\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    api_key=GROQ_API_KEY\n",
    ")\n",
    "model_with_tools = llm.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e4b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb7acf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
